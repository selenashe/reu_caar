{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4d5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 14:55:07.345246: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-31 14:55:19.138924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/nfshomes/dyang5/CAAR/lib/python3.11/site-packages/semdiffusers/pipeline_latent_edit_diffusion.py:12: FutureWarning: Importing `DiffusionPipeline` or `ImagePipelineOutput` from diffusers.pipeline_utils is deprecated. Please import from diffusers.pipelines.pipeline_utils instead.\n",
      "  from diffusers.pipeline_utils import DiffusionPipeline\n",
      "/nfshomes/dyang5/CAAR/lib/python3.11/site-packages/semdiffusers/pipeline_latent_edit_diffusion.py:79: UserWarning: You have disabled the safety checker for <class 'semdiffusers.pipeline_latent_edit_diffusion.SemanticEditPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from semdiffusers import SemanticEditPipeline\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device='cuda'\n",
    "\n",
    "pipe = SemanticEditPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    \n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "gen = torch.Generator(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def image_grid(imgs, rows, cols, spacing = 20):\n",
    "    assert len(imgs) == rows * cols\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    \n",
    "    grid = Image.new('RGBA', size=(cols * w + (cols-1)*spacing, rows * h + (rows-1)*spacing ), color=(255,255,255,0))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=( i // rows * (w+spacing), i % rows * (h+spacing)))\n",
    "        #print(( i // rows * w, i % rows * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99372eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "prompt = 'a portrait photo of a doctor'\n",
    "gen.manual_seed(seed)\n",
    "out = pipe(prompt=[prompt], negative_prompt = [\"deformed, blurry\"], generator=gen, num_images_per_prompt=5, guidance_scale=7.5)\n",
    "orig_imgs = out.images\n",
    "\n",
    "display(image_grid(orig_imgs, 1, len(orig_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_directory = 'images/sd_generated/'\n",
    "cropped_image_save_path = 'images/cropped/'\n",
    "\n",
    "for i in range(len(orig_imgs)):\n",
    "    image = orig_imgs[i]\n",
    "    image.save(f'{img_directory}doctor1_{i}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from deepface import DeepFace\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "def crop_faces(original_image_name, cropped_image_save_path):\n",
    "    '''\n",
    "    Take an image, detect faces using yolov8 (face), and crop each face.\n",
    "    '''\n",
    "    # Load a model\n",
    "    model = YOLO('yolov8n-face.pt')  # load an official model\n",
    "\n",
    "    # Predict with the model\n",
    "    results = model(f'{original_image_name}')  # predict on an image\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Boxes object for bbox outputs\n",
    "        masks = result.masks  # Masks object for segmentation masks outputs\n",
    "\n",
    "    box_amount = len(boxes)\n",
    "\n",
    "    xyxy_coordinates = boxes.xyxy # Get xyxy coordinates of bounded boxes\n",
    "\n",
    "    left, top, right, bottom = [], [], [], [] \n",
    "\n",
    "    for xyxy in xyxy_coordinates:\n",
    "        xyxy = xyxy.cpu().numpy()\n",
    "        left.append(xyxy[0])\n",
    "        top.append(xyxy[1])\n",
    "        right.append(xyxy[2])\n",
    "        bottom.append(xyxy[3])\n",
    "        \n",
    "\n",
    "    im = Image.open(f'{original_image_name}')\n",
    "    im = im.convert('RGB')\n",
    "\n",
    "    for i in range(box_amount):\n",
    "        if boxes[i].conf > 0.5: # confirm face confidence > 0.5\n",
    "            im_new = im.crop((left[i], top[i], right[i], bottom[i]))\n",
    "            current_image_name = os.path.basename(os.path.normpath(original_image_name))\n",
    "            if current_image_name.endswith('.jpg'):\n",
    "                current_image_name = current_image_name.removesuffix('.jpg')\n",
    "\n",
    "            im_new.save(f'{cropped_image_save_path}{current_image_name}_{i}.jpg')\n",
    "            \n",
    "def classify_images(cropped_path, classifier_output):\n",
    "    '''\n",
    "    For each image in directory, run DeepFace classifier and store output in row of dataframe.\n",
    "    \n",
    "        cropped_path: path to cropped images\n",
    "        classifier_output: dataframe that stores classifier results\n",
    "    '''\n",
    "    \n",
    "    classifier_results = {}\n",
    "    idx = 0\n",
    "    for file in os.listdir(cropped_path):\n",
    "        objs = DeepFace.analyze(img_path = cropped_path + \"/\" + file, \n",
    "                actions = ['gender', 'race'], enforce_detection = False\n",
    "        )\n",
    "\n",
    "        name = file\n",
    "        classifier_results[name] = [objs[0]['dominant_gender'], objs[0]['gender'], objs[0]['dominant_race'], objs[0]['race']]\n",
    "\n",
    "        # set up row in df\n",
    "        row = pd.DataFrame({\"file\": name,\n",
    "                            \"dominant_gender\": [objs[0]['dominant_gender']], \n",
    "                            \"Man\" : [objs[0]['gender']['Man']],\n",
    "                            \"Woman\" : [objs[0]['gender']['Woman']],\n",
    "                            \"dominant_race\" : [objs[0]['dominant_race']],\n",
    "                            \"white\": [objs[0]['race']['white']],\n",
    "                            \"latino hispanic\": [objs[0]['race']['latino hispanic']],\n",
    "                            \"asian\": [objs[0]['race']['asian']],\n",
    "                            \"black\": [objs[0]['race']['black']],\n",
    "                            \"middle eastern\": [objs[0]['race']['middle eastern']],\n",
    "                            \"indian\": [objs[0]['race']['indian']]},                        \n",
    "                            index = [idx])     \n",
    "        classifier_output = pd.concat([classifier_output, row])\n",
    "        output[name] = {# \"dominant_gender\": objs[0]['dominant_gender'],\n",
    "                        \"Man\" : objs[0]['gender']['Man'],\n",
    "                        \"Woman\" : objs[0]['gender']['Woman'],\n",
    "                        # \"dominant_race\" : objs[0]['dominant_race'],\n",
    "                        \"white\": objs[0]['race']['white'],\n",
    "                        \"latino hispanic\": objs[0]['race']['latino hispanic'],\n",
    "                        \"asian\": objs[0]['race']['asian'],\n",
    "                        \"black\": objs[0]['race']['black'],\n",
    "                        \"middle eastern\": objs[0]['race']['middle eastern'],\n",
    "                        \"indian\": objs[0]['race']['indian']}\n",
    "        idx += 1    \n",
    "        \n",
    "        # export results to csv\n",
    "        classifier_output = classifier_output.sort_values(by = 'file')\n",
    "        classifier_output.to_csv(\"pipeline_output.csv\", index = False)        \n",
    "        \n",
    "def main():\n",
    "    # crop images and store in path\n",
    "    for filename in os.listdir(img_directory):\n",
    "        file = os.path.join(img_directory, filename)\n",
    "        if os.path.isfile(file):\n",
    "            crop_faces(file, cropped_image_save_path)\n",
    "    \n",
    "    # set up dataframe to store results\n",
    "    classifier_output = pd.DataFrame(columns = ['file','dominant_gender', 'Man', 'Woman', \n",
    "                                 'dominant_race', 'white', 'latino hispanic', 'asian', \n",
    "                                 'black', 'middle eastern', 'indian'])\n",
    "    \n",
    "    classify_images(cropped_image_save_path, classifier_output)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair generation, without classifier\n",
    "\n",
    "target = {'editing_prompt': ['woman', 'man'], 'reverse_editing_direction':[False, True], \n",
    "          'edit_warmup_steps':10, 'edit_guidance_scale':6, 'edit_threshold':0.95, 'edit_momentum_scale':0.5, 'edit_mom_beta': 0.6}\n",
    "\n",
    "gen.manual_seed(seed)\n",
    "out = pipe(prompt=[prompt], negative_prompt = [\"deformed, blurry\"], generator=gen, num_images_per_prompt=5, guidance_scale=7.5,\n",
    "       **target)\n",
    "no_classifier_fair_imgs = out.images\n",
    "\n",
    "display(image_grid(no_classifier_fair_imgs, 1, len(no_classifier_fair_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, res in output.items():\n",
    "    for attribute, confidence in res.items():\n",
    "        print(str(file) + \" \" + str(attribute) + \" \" + str(confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_categories = ['male', 'female', 'white', 'latino hispanic', 'asian', \n",
    "                       'african american', 'middle eastern', 'indian']\n",
    "total_guidance = np.zeros(len(guidance_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cfa6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, res in output.items():\n",
    "    idx = 0\n",
    "    for attribute, confidence in res.items():\n",
    "        total_guidance[idx] += confidence\n",
    "        idx += 1\n",
    "\n",
    "avg_guidance = total_guidance / len(output.items())\n",
    "print(avg_guidance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b74c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair generation, with classifier\n",
    "\n",
    "weights = np.zeros(len(avg_guidance))\n",
    "edit_direction = np.zeros(len(avg_guidance), dtype = bool)\n",
    "\n",
    "for i in range(len(avg_guidance)):\n",
    "    weights[i] = 1/avg_guidance[i]\n",
    "\n",
    "sum_gender = 0\n",
    "sum_race = 0\n",
    "\n",
    "for i in range(len(avg_guidance)):\n",
    "    if i == 0 or i == 1:\n",
    "        sum_gender += weights[i]\n",
    "    else:\n",
    "        sum_race += weights[i]\n",
    "\n",
    "for i in range(len(avg_guidance)):\n",
    "    if i == 0 or i == 1:\n",
    "        weights[i] = weights[i]/sum_gender\n",
    "    else:\n",
    "        weights[i] = weights[i]/sum_race\n",
    "        \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6700c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {'editing_prompt': ['male', 'female'], 'reverse_editing_direction': [False, False], \n",
    "          'edit_warmup_steps':20, 'edit_guidance_scale':6, 'edit_threshold':0.95, 'edit_momentum_scale':0.5, 'edit_mom_beta': 0.6,\n",
    "          'edit_weights': [weights[0], weights[1]]\n",
    "         }\n",
    "\n",
    "gen.manual_seed(seed)\n",
    "out = pipe(prompt=[prompt], negative_prompt = [\"deformed, blurry\"], generator=gen, num_images_per_prompt=5, guidance_scale=7,\n",
    "       **target)\n",
    "imgs = out.images\n",
    "\n",
    "display(image_grid(imgs, 1, len(imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84749551",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image_grid(orig_imgs, 1, len(orig_imgs)))\n",
    "display(image_grid(no_classifier_fair_imgs, 1, len(no_classifier_fair_imgs)))\n",
    "display(image_grid(imgs, 1, len(imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de88caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guidance_categories)\n",
    "print(edit_direction)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = []\n",
    "for val in weights:\n",
    "    weight_list.append(val)\n",
    "    \n",
    "print(weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96718e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "target = {'editing_prompt': ['male', 'female', 'white', 'latino hispanic', 'asian', 'african american', 'middle eastern', 'indian'], 'reverse_editing_direction': [False, False, False, False, False, False, False, False], \n",
    "          'edit_warmup_steps':20, 'edit_guidance_scale':6, 'edit_threshold':0.95, 'edit_momentum_scale':0.5, 'edit_mom_beta': 0.6,\n",
    "          'edit_weights': [0.0010393328167246094, 0.9989606671832753, 0.016638013200516496, 0.02916591954029138, 0.6233491538562093, 0.037616209290234584, 0.027372628720911216, 0.2658580753918369]\n",
    "         }\n",
    "\n",
    "gen.manual_seed(42)\n",
    "out = pipe(prompt=['a portrait photo of a doctor'], negative_prompt = [\"deformed, blurry\"], generator=gen, num_images_per_prompt=5, guidance_scale=7,\n",
    "       **target)\n",
    "imgs = out.images\n",
    "\n",
    "display(image_grid(imgs, 1, len(imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a918eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
