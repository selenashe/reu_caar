{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92afcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dd5eea133e4405a9985fbaf6e09cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 face, 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b66747787840a7ab9b2b4c3d6ef045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 face, 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad970162e73946c8baabc352e97f81a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 face, 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbf261261a54595a55fe4575ac78d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 face, 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6515f5360a674405bd4d2ab7d8c6709f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 face, 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from deepface import DeepFace\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from semdiffusers import SemanticEditPipeline\n",
    "import torch\n",
    "\n",
    "# generating images\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device='cuda'\n",
    "pipe = SemanticEditPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\").to(device)\n",
    "gen = torch.Generator(device=device)\n",
    "\n",
    "save_path = \"images/python_generated/\"\n",
    "save_path_orig = \"images/python_generated_orig/\"\n",
    "prompt = \"a portrait photo of a doctor\"\n",
    "\n",
    "num_images = 5\n",
    "\n",
    "idx, num_generated = 0, 0\n",
    "while num_generated < num_images:\n",
    "    gen.manual_seed(idx)\n",
    "    params = {'guidance_scale': 7.5,\n",
    "              'seed': idx,\n",
    "              'prompt': prompt,\n",
    "              'negative_prompt': \"deformed, blurry\",\n",
    "              'num_images_per_prompt': 1\n",
    "             }\n",
    "    out = pipe(**params, generator=gen)\n",
    "    image = out.images[0]\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # if there is a face\n",
    "    model = YOLO('yolov8n-face.pt')\n",
    "    results = model(image)\n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Boxes object for bbox outputs\n",
    "        masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    box_amount = len(boxes)\n",
    "    \n",
    "    xyxy_coordinates = boxes.xyxy # Get xyxy coordinates of bounded boxes\n",
    "\n",
    "    left, top, right, bottom = [], [], [], [] \n",
    "\n",
    "    for xyxy in xyxy_coordinates:\n",
    "        xyxy = xyxy.cpu().numpy()\n",
    "        left.append(xyxy[0])\n",
    "        top.append(xyxy[1])\n",
    "        right.append(xyxy[2])\n",
    "        bottom.append(xyxy[3])    \n",
    "    \n",
    "    for i in range(box_amount):\n",
    "        if boxes[i].conf > 0.5: # confirm face confidence > 0.5\n",
    "            image.save(f\"{save_path_orig}image{idx}.jpg\")\n",
    "            im_new = image.crop((left[i], top[i], right[i], bottom[i]))\n",
    "            im_new.save(f\"{save_path}image{idx}.jpg\")\n",
    "            num_generated += 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ea9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|███████████████████████████████| 2/2 [00:02<00:00,  1.35s/it]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.27it/s]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.26it/s]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.27it/s]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.28it/s]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.28it/s]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.26it/s]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.19it/s]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.26it/s]\n",
      "Action: race: 100%|███████████████████████████████| 2/2 [00:00<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "img_directory = 'images/python_generated/'\n",
    "cropped_image_save_path = 'images/python_generated/'\n",
    "output = {}\n",
    "    \n",
    "def classify_images(cropped_path, classifier_output):\n",
    "    '''\n",
    "    For each image in directory, run DeepFace classifier and store output in row of dataframe.\n",
    "    \n",
    "        cropped_path: path to cropped images\n",
    "        classifier_output: dataframe that stores classifier results\n",
    "    '''\n",
    "    \n",
    "    classifier_results = {}\n",
    "    idx = 0\n",
    "    for file in os.listdir(cropped_path):\n",
    "        objs = DeepFace.analyze(img_path = cropped_path + \"/\" + file, \n",
    "                actions = ['gender', 'race'], enforce_detection = False\n",
    "        )\n",
    "\n",
    "        name = file\n",
    "        classifier_results[name] = [objs[0]['dominant_gender'], objs[0]['gender'], objs[0]['dominant_race'], objs[0]['race']]\n",
    "\n",
    "        # set up row in df\n",
    "        row = pd.DataFrame({\"file\": name,\n",
    "                            \"dominant_gender\": [objs[0]['dominant_gender']], \n",
    "                            \"Man\" : [objs[0]['gender']['Man']],\n",
    "                            \"Woman\" : [objs[0]['gender']['Woman']],\n",
    "                            \"dominant_race\" : [objs[0]['dominant_race']],\n",
    "                            \"white\": [objs[0]['race']['white']],\n",
    "                            \"latino hispanic\": [objs[0]['race']['latino hispanic']],\n",
    "                            \"asian\": [objs[0]['race']['asian']],\n",
    "                            \"black\": [objs[0]['race']['black']],\n",
    "                            \"middle eastern\": [objs[0]['race']['middle eastern']],\n",
    "                            \"indian\": [objs[0]['race']['indian']]},                        \n",
    "                            index = [idx])     \n",
    "        classifier_output = pd.concat([classifier_output, row])\n",
    "        output[name] = {# \"dominant_gender\": objs[0]['dominant_gender'],\n",
    "                        \"Man\" : objs[0]['gender']['Man'],\n",
    "                        \"Woman\" : objs[0]['gender']['Woman'],\n",
    "                        # \"dominant_race\" : objs[0]['dominant_race'],\n",
    "                        \"white\": objs[0]['race']['white'],\n",
    "                        \"latino hispanic\": objs[0]['race']['latino hispanic'],\n",
    "                        \"asian\": objs[0]['race']['asian'],\n",
    "                        \"black\": objs[0]['race']['black'],\n",
    "                        \"middle eastern\": objs[0]['race']['middle eastern'],\n",
    "                        \"indian\": objs[0]['race']['indian']}\n",
    "        idx += 1    \n",
    "        \n",
    "        # export results to csv\n",
    "        classifier_output = classifier_output.sort_values(by = 'file')\n",
    "        classifier_output.to_csv(\"pipeline_output.csv\", index = False)        \n",
    "        \n",
    "def main():\n",
    "    # crop images and store in path\n",
    "    for filename in os.listdir(img_directory):\n",
    "        file = os.path.join(img_directory, filename)\n",
    "    \n",
    "    # set up dataframe to store results\n",
    "    classifier_output = pd.DataFrame(columns = ['file','dominant_gender', 'Man', 'Woman', \n",
    "                                 'dominant_race', 'white', 'latino hispanic', 'asian', \n",
    "                                 'black', 'middle eastern', 'indian'])\n",
    "    \n",
    "    classify_images(cropped_image_save_path, classifier_output)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bffc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
